{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b3b67",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd541805",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Latex Paper\\graphics\\chapter1\\vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e540fd2",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f169b",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3135b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json                       #visualisation\n",
    "import matplotlib.pyplot as plt             #visualisation\n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc410fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Dataset/VQA V2/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579c3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       question_type multiple_choice_answer  \\\n",
      "0       what is this                    net   \n",
      "1               what                pitcher   \n",
      "2  what color is the                 orange   \n",
      "3            is this                    yes   \n",
      "4  what color is the                  white   \n",
      "\n",
      "                                             answers  image_id answer_type  \\\n",
      "0  [{'answer': 'net', 'answer_confidence': 'maybe...    458752       other   \n",
      "1  [{'answer': 'pitcher', 'answer_confidence': 'y...    458752       other   \n",
      "2  [{'answer': 'orange', 'answer_confidence': 'ye...    458752       other   \n",
      "3  [{'answer': 'yes', 'answer_confidence': 'yes',...    458752      yes/no   \n",
      "4  [{'answer': 'white', 'answer_confidence': 'yes...    262146       other   \n",
      "\n",
      "   question_id  \n",
      "0    458752000  \n",
      "1    458752001  \n",
      "2    458752002  \n",
      "3    458752003  \n",
      "4    262146000  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# đọc JSON\n",
    "with open('Dataset/VQA V2/v2_Annotations_Train_mscoco/v2_mscoco_train2014_annotations.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data[\"annotations\"])\n",
    "\n",
    "print(df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA CLIP",
   "language": "python",
   "name": "vqa_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
