{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b3b67",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd541805",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Latex Paper\\graphics\\chapter1\\vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e540fd2",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f169b",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3135b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing os, numpy and pandas for data manipulation\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization, we will use matplotlib, wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For data preprocessing, we will use Counter, train_test_split, Levenshtein distance, Python Image Library and OneHotEncoder\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For saving and loading the preprocessed data, we will use pickle\n",
    "import pickle\n",
    "\n",
    "# For Building the model, we will use PyTorch and its functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For taking the image from the URL, we will use requests\n",
    "import requests\n",
    "\n",
    "# For evaluation, we will need sklearn.metrics.average_precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Importing json for results formatting which will be uploaded for evaluation\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d621fb",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09b53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Configuring the paths for the dataset\n",
    "INPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Dataset\\VizWiz-'\n",
    "ANNOTATIONS = INPUT_PATH + '/Annotations'\n",
    "TRAIN_PATH = INPUT_PATH + '/train/train'\n",
    "VALIDATION_PATH = INPUT_PATH + '/val/val'\n",
    "ANNOTATIONS_TRAIN_PATH = ANNOTATIONS + '/train.json'\n",
    "ANNOTATIONS_VAL_PATH = ANNOTATIONS + '/val.json'\n",
    "ANNOTATIONS_TEST_PATH = ANNOTATIONS + '/test.json'\n",
    "TEST_PATH = INPUT_PATH + '/test/test'\n",
    "\n",
    "OUTPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Output'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "\n",
    "# Using accelerated computing if available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9ef73",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "579c3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  VizWiz_train_00000000.jpg   \n",
      "1  VizWiz_train_00000001.jpg   \n",
      "2  VizWiz_train_00000002.jpg   \n",
      "3  VizWiz_train_00000003.jpg   \n",
      "4  VizWiz_train_00000004.jpg   \n",
      "\n",
      "                                            question  \\\n",
      "0                   What's the name of this product?   \n",
      "1        Can you tell me what is in this can please?   \n",
      "2  Is this enchilada sauce or is this tomatoes?  ...   \n",
      "3            What is the captcha on this screenshot?   \n",
      "4                                 What is this item?   \n",
      "\n",
      "                                             answers answer_type  answerable  \n",
      "0  [{'answer_confidence': 'yes', 'answer': 'basil...       other           1  \n",
      "1  [{'answer_confidence': 'yes', 'answer': 'soda'...       other           1  \n",
      "2  [{'answer_confidence': 'yes', 'answer': 'these...       other           1  \n",
      "3  [{'answer_confidence': 'yes', 'answer': 't36m'...       other           1  \n",
      "4  [{'answer_confidence': 'yes', 'answer': 'solar...       other           1  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# đọc JSON\n",
    "with open(ANNOTATIONS_TRAIN_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12d5e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "image          object\n",
       "question       object\n",
       "answers        object\n",
       "answer_type    object\n",
       "answerable      int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9872e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           image  \\\n",
      "0      VizWiz_train_00000000.jpg   \n",
      "1      VizWiz_train_00000001.jpg   \n",
      "2      VizWiz_train_00000002.jpg   \n",
      "3      VizWiz_train_00000003.jpg   \n",
      "4      VizWiz_train_00000004.jpg   \n",
      "...                          ...   \n",
      "20518  VizWiz_train_00023949.jpg   \n",
      "20519  VizWiz_train_00023950.jpg   \n",
      "20520  VizWiz_train_00023951.jpg   \n",
      "20521  VizWiz_train_00023952.jpg   \n",
      "20522  VizWiz_train_00023953.jpg   \n",
      "\n",
      "                                                question  \\\n",
      "0                       What's the name of this product?   \n",
      "1            Can you tell me what is in this can please?   \n",
      "2      Is this enchilada sauce or is this tomatoes?  ...   \n",
      "3                What is the captcha on this screenshot?   \n",
      "4                                     What is this item?   \n",
      "...                                                  ...   \n",
      "20518                  What's the color for this laptop?   \n",
      "20519  (inaudible) can you see it? If so, then tell m...   \n",
      "20520         What are the three numbers I have to type?   \n",
      "20521                                       Is it a box?   \n",
      "20522                      What is this in this picture?   \n",
      "\n",
      "                                                 answers   answer_type  \\\n",
      "0      [{'answer_confidence': 'yes', 'answer': 'basil...         other   \n",
      "1      [{'answer_confidence': 'yes', 'answer': 'soda'...         other   \n",
      "2      [{'answer_confidence': 'yes', 'answer': 'these...         other   \n",
      "3      [{'answer_confidence': 'yes', 'answer': 't36m'...         other   \n",
      "4      [{'answer_confidence': 'yes', 'answer': 'solar...         other   \n",
      "...                                                  ...           ...   \n",
      "20518  [{'answer': 'grey black', 'answer_confidence':...         other   \n",
      "20519  [{'answer': 'yes', 'answer_confidence': 'maybe...         other   \n",
      "20520  [{'answer': '321', 'answer_confidence': 'no'},...  unanswerable   \n",
      "20521  [{'answer': 'no', 'answer_confidence': 'yes'},...        yes/no   \n",
      "20522  [{'answer': 'car', 'answer_confidence': 'yes'}...         other   \n",
      "\n",
      "       answerable  \n",
      "0               1  \n",
      "1               1  \n",
      "2               1  \n",
      "3               1  \n",
      "4               1  \n",
      "...           ...  \n",
      "20518           1  \n",
      "20519           1  \n",
      "20520           0  \n",
      "20521           1  \n",
      "20522           1  \n",
      "\n",
      "[20523 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and returns a dataframe with available columns\n",
    "    (image, question, answers, answer_type, answerable if exist)\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Dataframe with the available columns\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "\n",
    "    cols = ['image', 'question', 'answers', 'answer_type', 'answerable']\n",
    "\n",
    "    existing_cols = [c for c in cols if c in df.columns]\n",
    "    df = df[existing_cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be split\n",
    "\n",
    "    Returns:\n",
    "        train (pandas.DataFrame): Train set\n",
    "        test (pandas.DataFrame): Test set\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "def plot_histogram(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the histogram of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.hist(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pie(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the pie chart of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    value_counts = dataframe[column].value_counts()\n",
    "    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the wordcloud of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in dataframe[column]])\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    min_font_size = 10).generate(text) \n",
    "    \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "def explore_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Explores the dataframe (EDA) by plotting the pie charts, histograms and wordclouds of the columns\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plot_pie(dataframe, 'answer_type')\n",
    "    plot_pie(dataframe, 'answerable')\n",
    "    plot_histogram(dataframe, 'answerable')\n",
    "    plot_wordcloud(dataframe, 'question')\n",
    "    \n",
    "def get_number_of_distinct_answers(dataframe):\n",
    "    \"\"\"\n",
    "    Returns the number of distinct answers in the dataframe\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        len(unique_answers_set) (int): Number of distinct answers in the dataframe\n",
    "    \"\"\"\n",
    "    unique_answers_set = set()\n",
    "    for row in dataframe['answers']:\n",
    "        for answer_map in row:\n",
    "            unique_answers_set.add(answer_map['answer'])\n",
    "    return len(unique_answers_set)\n",
    "\n",
    "def process_images(dataframe, image_path, clip_model, preprocessor, device):\n",
    "    \"\"\"\n",
    "    Processes the images in the dataframe and returns the image features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the images\n",
    "        image_path (str): Path to the input images\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        preprocessor (clip.model.Preprocess): Preprocessor for the CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "    \n",
    "    Returns:\n",
    "        images (list): List of image features\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        full_path = image_path + \"/\" + row['image']\n",
    "        image = Image.open(full_path)\n",
    "        image = preprocessor(image).unsqueeze(0).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        image_features = torch.flatten(image_features, start_dim=1)\n",
    "        images.append(image_features)\n",
    "    return images\n",
    "\n",
    "def process_questions(dataframe, clip_model,device):\n",
    "    \"\"\"\n",
    "    Processes the questions in the dataframe and returns the question features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the questions\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        questions (list): List of question features\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['question']\n",
    "        question =  clip.tokenize(question).to(device)\n",
    "        text_features = clip_model.encode_text(question).float()\n",
    "        text_features = torch.flatten(text_features, start_dim=1)\n",
    "        questions.append(text_features)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a354d",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creating Dataframes & Splitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa55f7",
   "metadata": {},
   "source": [
    "Ussing the defined function to create dataframes and split them into train and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33eb21fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['answers', 'answer_type', 'answerable'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m read_dataframe(ANNOTATIONS_TRAIN_PATH)\n\u001b[0;32m      2\u001b[0m validation_df \u001b[38;5;241m=\u001b[39m read_dataframe(ANNOTATIONS_VAL_PATH)\n\u001b[1;32m----> 3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[43mread_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mANNOTATIONS_TEST_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m ANSWER_SPACE \u001b[38;5;241m=\u001b[39m get_number_of_distinct_answers(train_df) \u001b[38;5;66;03m# The answer space will be decreased later when we process the answers\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of distinct answers: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ANSWER_SPACE)\n",
      "Cell \u001b[1;32mIn[14], line 12\u001b[0m, in \u001b[0;36mread_dataframe\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mReads the JSON file and returns a dataframe with the required columns (image, question, answers, answer_type, answerable)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    df (pandas.DataFrame): Dataframe with the required columns\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(path)\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswer_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manswerable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4112\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4113\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4115\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6209\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6210\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6212\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6214\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6216\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6263\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6264\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['answers', 'answer_type', 'answerable'] not in index\""
     ]
    }
   ],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "test_df = read_dataframe(ANNOTATIONS_TEST_PATH)\n",
    "ANSWER_SPACE = get_number_of_distinct_answers(train_df) # The answer space will be decreased later when we process the answers\n",
    "print(\"Number of distinct answers: \", ANSWER_SPACE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4116fc",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Exploratory Data Analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed37007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (19496, 5)\n",
      "\n",
      "Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 19496 entries, 14709 to 8401\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   image        19496 non-null  object\n",
      " 1   question     19496 non-null  object\n",
      " 2   answers      19496 non-null  object\n",
      " 3   answer_type  19496 non-null  object\n",
      " 4   answerable   19496 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 913.9+ KB\n",
      "None\n",
      "\n",
      "Missing values:\n",
      " image          0\n",
      "question       0\n",
      "answers        0\n",
      "answer_type    0\n",
      "answerable     0\n",
      "dtype: int64\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mexplore_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 88\u001b[0m, in \u001b[0;36mexplore_dataframe\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataframe\u001b[38;5;241m.\u001b[39minfo())\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMissing values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[1;32m---> 88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDuplicate rows:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mdataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Pie charts\u001b[39;00m\n\u001b[0;32m     91\u001b[0m plot_pie(dataframe, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manswer_type\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:6965\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6964\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[1;32m-> 6965\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   6967\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(labels, \u001b[38;5;28mtuple\u001b[39m(shape), sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   6968\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:6933\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[1;34m(vals)\u001b[0m\n\u001b[0;32m   6932\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mf\u001b[39m(vals) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m-> 6933\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6934\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi8\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m), \u001b[38;5;28mlen\u001b[39m(shape)\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\algorithms.py:795\u001b[0m, in \u001b[0;36mfactorize\u001b[1;34m(values, sort, use_na_sentinel, size_hint)\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[38;5;66;03m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[0;32m    793\u001b[0m             values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[1;32m--> 795\u001b[0m     codes, uniques \u001b[38;5;241m=\u001b[39m \u001b[43mfactorize_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_na_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sort \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    802\u001b[0m     uniques, codes \u001b[38;5;241m=\u001b[39m safe_sort(\n\u001b[0;32m    803\u001b[0m         uniques,\n\u001b[0;32m    804\u001b[0m         codes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    807\u001b[0m         verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    808\u001b[0m     )\n",
      "File \u001b[1;32me:\\HK1  2025-2026\\Đồ án CS420\\Visual-Question-Answering\\.venv\\lib\\site-packages\\pandas\\core\\algorithms.py:595\u001b[0m, in \u001b[0;36mfactorize_array\u001b[1;34m(values, use_na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[0;32m    592\u001b[0m hash_klass, values \u001b[38;5;241m=\u001b[39m _get_hashtable_algo(values)\n\u001b[0;32m    594\u001b[0m table \u001b[38;5;241m=\u001b[39m hash_klass(size_hint \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(values))\n\u001b[1;32m--> 595\u001b[0m uniques, codes \u001b[38;5;241m=\u001b[39m \u001b[43mtable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_sentinel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_na_sentinel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;66;03m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[0;32m    604\u001b[0m uniques \u001b[38;5;241m=\u001b[39m _reconstruct_data(uniques, original\u001b[38;5;241m.\u001b[39mdtype, original)\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7293\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas/_libs/hashtable_class_helper.pxi:7203\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA CLIP",
   "language": "python",
   "name": "vqa_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
