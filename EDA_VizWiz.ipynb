{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b3b67",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd541805",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Latex Paper\\graphics\\chapter1\\vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e540fd2",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f169b",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3135b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import json                       \n",
    "import matplotlib.pyplot as plt             \n",
    "%matplotlib inline     \n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d621fb",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a09b53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Configuring the paths for the dataset\n",
    "INPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Dataset\\VizWiz-'\n",
    "ANNOTATIONS = INPUT_PATH + '/Annotations'\n",
    "TRAIN_PATH = INPUT_PATH + '/train/train'\n",
    "VALIDATION_PATH = INPUT_PATH + '/val/val'\n",
    "ANNOTATIONS_TRAIN_PATH = ANNOTATIONS + '/train.json'\n",
    "ANNOTATIONS_VAL_PATH = ANNOTATIONS + '/val.json'\n",
    "OUTPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Output'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "# Using accelerated computing if available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9ef73",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579c3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  VizWiz_train_00000000.jpg   \n",
      "1  VizWiz_train_00000001.jpg   \n",
      "2  VizWiz_train_00000002.jpg   \n",
      "3  VizWiz_train_00000003.jpg   \n",
      "4  VizWiz_train_00000004.jpg   \n",
      "\n",
      "                                            question  \\\n",
      "0                   What's the name of this product?   \n",
      "1        Can you tell me what is in this can please?   \n",
      "2  Is this enchilada sauce or is this tomatoes?  ...   \n",
      "3            What is the captcha on this screenshot?   \n",
      "4                                 What is this item?   \n",
      "\n",
      "                                             answers answer_type  answerable  \n",
      "0  [{'answer_confidence': 'yes', 'answer': 'basil...       other           1  \n",
      "1  [{'answer_confidence': 'yes', 'answer': 'soda'...       other           1  \n",
      "2  [{'answer_confidence': 'yes', 'answer': 'these...       other           1  \n",
      "3  [{'answer_confidence': 'yes', 'answer': 't36m'...       other           1  \n",
      "4  [{'answer_confidence': 'yes', 'answer': 'solar...       other           1  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# đọc JSON\n",
    "with open(ANNOTATIONS_TRAIN_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12d5e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type             object\n",
       "multiple_choice_answer    object\n",
       "answers                   object\n",
       "image_id                   int64\n",
       "answer_type               object\n",
       "question_id                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other' 'yes/no' 'number']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362e4f7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA CLIP",
   "language": "python",
   "name": "vqa_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
