{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047b3b67",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Visual Question Answering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd541805",
   "metadata": {},
   "source": [
    "Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. VQA has many applications: Medical VQA, Education purposes, for surveillance and numerous other applications. In this project we will use [VizWiz](https://vizwiz.org/tasks-and-datasets/vqa/) dataset for Visual Question Answering, this dataset was constructed to train models to help visually impaired people.  In the words of creators of VizWiz: “we introduce the visual question answering (VQA) dataset coming from this population, which we call VizWiz-VQA.  It originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, together with 10 crowdsourced answers per visual question.”\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Latex Paper\\graphics\\chapter1\\vizwiz_example.png\" alt=\"vizwiz_example\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e540fd2",
   "metadata": {},
   "source": [
    "- **Note:** This repository is an implementation for [Less is More: Linear Layers on CLIP Features as Powerful VizWiz Model](https://arxiv.org/abs/2206.05281) paper.\n",
    "- It is really advised to read OpenAI's [CLIP](https://openai.com/blog/clip/) paper before reading this repository if you have enough time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0f169b",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Installing Required Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3135b235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing os, numpy and pandas for data manipulation\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# For data visualization, we will use matplotlib, wordcloud\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# For data preprocessing, we will use Counter, train_test_split, Levenshtein distance, Python Image Library and OneHotEncoder\n",
    "from collections import Counter\n",
    "import Levenshtein as lev\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For saving and loading the preprocessed data, we will use pickle\n",
    "import pickle\n",
    "\n",
    "# For Building the model, we will use PyTorch and its functions\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import clip\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For taking the image from the URL, we will use requests\n",
    "import requests\n",
    "\n",
    "# For evaluation, we will need sklearn.metrics.average_precision_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# Importing json for results formatting which will be uploaded for evaluation\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d621fb",
   "metadata": {},
   "source": [
    "## <a id='toc1_3_'></a>[Configuring the Notebook](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09b53b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device: cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "# Configuring the paths for the dataset\n",
    "INPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Dataset\\VizWiz-'\n",
    "ANNOTATIONS = INPUT_PATH + '/Annotations'\n",
    "TRAIN_PATH = INPUT_PATH + '/train/train'\n",
    "VALIDATION_PATH = INPUT_PATH + '/val/val'\n",
    "ANNOTATIONS_TRAIN_PATH = ANNOTATIONS + '/train.json'\n",
    "ANNOTATIONS_VAL_PATH = ANNOTATIONS + '/val.json'\n",
    "OUTPUT_PATH = 'E:\\HK1  2025-2026\\Đồ án CS420\\VQA\\Output'\n",
    "ANSWER_SPACE = 0 # Will be configured later when we build the vocab using the methodology described in the paper\n",
    "MODEL_NAME = \"ViT-L/14@336px\" # This is the backbone of the CLIP model\n",
    "\n",
    "# Using accelerated computing if available\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device:\", torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf9ef73",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Processing Data](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "579c3483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       image  \\\n",
      "0  VizWiz_train_00000000.jpg   \n",
      "1  VizWiz_train_00000001.jpg   \n",
      "2  VizWiz_train_00000002.jpg   \n",
      "3  VizWiz_train_00000003.jpg   \n",
      "4  VizWiz_train_00000004.jpg   \n",
      "\n",
      "                                            question  \\\n",
      "0                   What's the name of this product?   \n",
      "1        Can you tell me what is in this can please?   \n",
      "2  Is this enchilada sauce or is this tomatoes?  ...   \n",
      "3            What is the captcha on this screenshot?   \n",
      "4                                 What is this item?   \n",
      "\n",
      "                                             answers answer_type  answerable  \n",
      "0  [{'answer_confidence': 'yes', 'answer': 'basil...       other           1  \n",
      "1  [{'answer_confidence': 'yes', 'answer': 'soda'...       other           1  \n",
      "2  [{'answer_confidence': 'yes', 'answer': 'these...       other           1  \n",
      "3  [{'answer_confidence': 'yes', 'answer': 't36m'...       other           1  \n",
      "4  [{'answer_confidence': 'yes', 'answer': 'solar...       other           1  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# đọc JSON\n",
    "with open(ANNOTATIONS_TRAIN_PATH, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12d5e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question_type             object\n",
       "multiple_choice_answer    object\n",
       "answers                   object\n",
       "image_id                   int64\n",
       "answer_type               object\n",
       "question_id                int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9872e99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['other' 'yes/no' 'number']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362e4f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataframe(path):\n",
    "    \"\"\"\n",
    "    Reads the JSON file and returns a dataframe with the required columns (image, question, answers, answer_type, answerable)\n",
    "\n",
    "    Parameters:\n",
    "        path (str): Path to the JSON file\n",
    "\n",
    "    Returns:\n",
    "        df (pandas.DataFrame): Dataframe with the required columns\n",
    "    \"\"\"\n",
    "    df = pd.read_json(path)\n",
    "    df = df[['image', 'question', 'answers', 'answer_type', 'answerable']]\n",
    "    return df\n",
    "\n",
    "def split_train_test(dataframe, test_size = 0.05):\n",
    "    \"\"\"\n",
    "    Splits the dataframe into train and test sets\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be split\n",
    "\n",
    "    Returns:\n",
    "        train (pandas.DataFrame): Train set\n",
    "        test (pandas.DataFrame): Test set\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(dataframe, test_size=test_size, random_state=42, stratify=dataframe[['answer_type', 'answerable']])\n",
    "    return train, test\n",
    "\n",
    "def plot_histogram(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the histogram of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.hist(dataframe[column])\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_pie(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the pie chart of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    value_counts = dataframe[column].value_counts()\n",
    "    plt.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "\n",
    "def plot_wordcloud(dataframe, column):\n",
    "    \"\"\"\n",
    "    Plots the wordcloud of the given column\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be plotted\n",
    "        column (str): Column to be plotted\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    text = \" \".join([word for word in dataframe[column]])\n",
    "\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    min_font_size = 10).generate(text) \n",
    "    \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    plt.show()\n",
    "\n",
    "def explore_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Explores the dataframe (EDA) by plotting the pie charts, histograms and wordclouds of the columns\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plot_pie(dataframe, 'answer_type')\n",
    "    plot_pie(dataframe, 'answerable')\n",
    "    plot_histogram(dataframe, 'answerable')\n",
    "    plot_wordcloud(dataframe, 'question')\n",
    "    \n",
    "def get_number_of_distinct_answers(dataframe):\n",
    "    \"\"\"\n",
    "    Returns the number of distinct answers in the dataframe\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe to be explored\n",
    "\n",
    "    Returns:\n",
    "        len(unique_answers_set) (int): Number of distinct answers in the dataframe\n",
    "    \"\"\"\n",
    "    unique_answers_set = set()\n",
    "    for row in dataframe['answers']:\n",
    "        for answer_map in row:\n",
    "            unique_answers_set.add(answer_map['answer'])\n",
    "    return len(unique_answers_set)\n",
    "\n",
    "def process_images(dataframe, image_path, clip_model, preprocessor, device):\n",
    "    \"\"\"\n",
    "    Processes the images in the dataframe and returns the image features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the images\n",
    "        image_path (str): Path to the input images\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        preprocessor (clip.model.Preprocess): Preprocessor for the CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "    \n",
    "    Returns:\n",
    "        images (list): List of image features\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        full_path = image_path + \"/\" + row['image']\n",
    "        image = Image.open(full_path)\n",
    "        image = preprocessor(image).unsqueeze(0).to(device)\n",
    "        image_features = clip_model.encode_image(image)\n",
    "        image_features = torch.flatten(image_features, start_dim=1)\n",
    "        images.append(image_features)\n",
    "    return images\n",
    "\n",
    "def process_questions(dataframe, clip_model,device):\n",
    "    \"\"\"\n",
    "    Processes the questions in the dataframe and returns the question features\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): Dataframe containing the questions\n",
    "        clip_model (clip.model.CLIP): CLIP model\n",
    "        device (torch.device): Device to be used for processing\n",
    "\n",
    "    Returns:\n",
    "        questions (list): List of question features\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    for _, row in dataframe.iterrows():\n",
    "        question = row['question']\n",
    "        question =  clip.tokenize(question).to(device)\n",
    "        text_features = clip_model.encode_text(question).float()\n",
    "        text_features = torch.flatten(text_features, start_dim=1)\n",
    "        questions.append(text_features)\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2a354d",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Creating Dataframes & Splitting](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa55f7",
   "metadata": {},
   "source": [
    "Ussing the defined function to create dataframes and split them into train and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33eb21fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct answers:  39512\n"
     ]
    }
   ],
   "source": [
    "train_df = read_dataframe(ANNOTATIONS_TRAIN_PATH)\n",
    "validation_df = read_dataframe(ANNOTATIONS_VAL_PATH)\n",
    "train_df, test_df = split_train_test(train_df, test_size=0.05)\n",
    "ANSWER_SPACE = get_number_of_distinct_answers(train_df) # The answer space will be decreased later when we process the answers\n",
    "print(\"Number of distinct answers: \", ANSWER_SPACE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4116fc",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Exploratory Data Analysis](#toc0_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VQA CLIP",
   "language": "python",
   "name": "vqa_clip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
